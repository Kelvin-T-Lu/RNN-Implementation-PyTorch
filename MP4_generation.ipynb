{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n",
      "train len:  1003854\n",
      "test len:  111540\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = 'language_data/shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue now, thou varlet; thou art to continue.\n",
      "\n",
      "ESCALUS:\n",
      "Where were you born, friend?\n",
      "\n",
      "FROTH:\n",
      "Here in Vienna, sir.\n",
      "\n",
      "ESCALUS:\n",
      "Are you of fourscore pounds a year?\n",
      "\n",
      "FROTH:\n",
      "Yes, an't please you, sir.\n",
      "\n",
      "ES\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.01\n",
    "model_type = 'rnn'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    h = rnn.init_hidden(batch_size, device)\n",
    "    rnn.zero_grad()\n",
    "    for c in range(chunk_len):\n",
    "        char = input[:, c]\n",
    "        output, h = rnn.forward(char, h)\n",
    "        \n",
    "        loss += criterion(output.view(batch_size, -1), target[:, c])\n",
    "    loss = loss / chunk_len\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##########       END      ##########\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n",
      "[0m 43s (50 1%) train loss: 2.4778, test_loss: 2.5042]\n",
      "Whore g thet plar way witimanghisoure\n",
      "O: tird, y hend\n",
      "Ando alerr the th, thesead t aruper Ror if kithi \n",
      "\n",
      "[1m 26s (100 2%) train loss: 2.4744, test_loss: 2.4795]\n",
      "Whe bry thotenco t leftot lss LA thithasseaw eape hesurofin rulast orier opor nd, be wous y wend mer f \n",
      "\n",
      "[2m 7s (150 3%) train loss: 2.4613, test_loss: 2.4849]\n",
      "Whe I rirs istl th:\n",
      "M: br the adear s br thike faveste, blllound not thot INGofuithotor,\n",
      "Cadren ind h  \n",
      "\n",
      "[2m 50s (200 4%) train loss: 2.4857, test_loss: 2.4870]\n",
      "Wheet allk's way olo y we s ld in wourir, anthes murat aldos henovemy s.\n",
      "Ander wite.\n",
      "Thaxe t a al be b \n",
      "\n",
      "[3m 33s (250 5%) train loss: 2.4702, test_loss: 2.4970]\n",
      "Whyor\n",
      "\n",
      "\n",
      "Tha tibr guelideat be but torthe cuthocoungot rere las hisor one wh bur;\n",
      "Benes y d fr,\n",
      "SCathin \n",
      "\n",
      "[4m 16s (300 6%) train loss: 2.4641, test_loss: 2.4942]\n",
      "Whou f hod he oce an,\n",
      "THor\n",
      "t wo be rll thisture fan, alisorindlise bllinou ses, fo I hint d at rarrind \n",
      "\n",
      "[4m 57s (350 7%) train loss: 2.4789, test_loss: 2.4952]\n",
      "Whe mm inou y sl merelat n CAULA:\n",
      "Tou the hthing ane lld for fre waive thin gare sl ne my me turare wh \n",
      "\n",
      "[5m 40s (400 8%) train loss: 2.4648, test_loss: 2.4902]\n",
      "Whin, sthee my, lyoues: ist t thy; athe FO, fak sthad hathe fo ile he ildager rind d l\n",
      "Clllldrethathe  \n",
      "\n",
      "[6m 22s (450 9%) train loss: 2.4635, test_loss: 2.5048]\n",
      "Wher s, breaithepeth the yoore my the ans hast mindito t plond be s us e to tr or gn he's,\n",
      "To ou my be \n",
      "\n",
      "[7m 7s (500 10%) train loss: 2.4632, test_loss: 2.4929]\n",
      "Wheeanou, m I ach tr acalisw fanoun thr t MERD:\n",
      "DUEve t it ind se shicit heaseceriey f th ll tes tinke \n",
      "\n",
      "[7m 52s (550 11%) train loss: 2.4595, test_loss: 2.4967]\n",
      "Whe yomit surncaisto s we, hasonay ind on wie.\n",
      "Whe ill tous. n ar ne wis and merthino cose hesu hewity \n",
      "\n",
      "[8m 36s (600 12%) train loss: 2.4582, test_loss: 2.4983]\n",
      "Whapereand ke an pllk's, f the ten and is an cen sur thenoute, tes, me, Cockll nd f bl tore whin pan,  \n",
      "\n",
      "[9m 22s (650 13%) train loss: 2.4495, test_loss: 2.4648]\n",
      "Whindsourion:\n",
      "Th thondinourofovenour t:\n",
      "Whou h t han y tighe, t es id f ave s anor ds d t wed ge hyoma \n",
      "\n",
      "[10m 5s (700 14%) train loss: 2.4475, test_loss: 2.4974]\n",
      "Whal ayo TI:\n",
      "Cof is thinouteinghayo ore tid s,\n",
      "ESARI ifreny fishoucor fo an harorenticent ices y pore  \n",
      "\n",
      "[10m 51s (750 15%) train loss: 2.4673, test_loss: 2.5065]\n",
      "Whel ar mo En g, t he by rer inofoule.\n",
      "Whe me!\n",
      "\n",
      "Wh,\n",
      "ARGres gh whethar.\n",
      "Heale be heagouloyserinou har g \n",
      "\n",
      "[11m 31s (800 16%) train loss: 2.4595, test_loss: 2.5058]\n",
      "Wh bowese ar he l at an ge pa inshiond ndon he the.\n",
      "Wisend gssteat bat? mein haloma belt y yonchy beat \n",
      "\n",
      "[12m 16s (850 17%) train loss: 2.4575, test_loss: 2.4868]\n",
      "Whinthil atig ouen, gh tart ie,\n",
      "Thover y welll wime llinothane's I wacorre the thethe ghatou we de tle \n",
      "\n",
      "[12m 57s (900 18%) train loss: 2.4506, test_loss: 2.4793]\n",
      "Whame ald urind Gerore aye wiveenily lend berser bul hithind, f myoncld.\n",
      "Ththing sto u se te I f the b \n",
      "\n",
      "[13m 42s (950 19%) train loss: 2.4576, test_loss: 2.5179]\n",
      "Where n llor w berowharenghe sh wins ndrse he fe de f: or, wandourestr emas.\n",
      "BUS: por ne bean grlf I d \n",
      "\n",
      "[14m 28s (1000 20%) train loss: 2.4642, test_loss: 2.4777]\n",
      "Whive th pl\n",
      "Cilldow---\n",
      "LINTorr t y'd d heran thind clld somagle, mbu us tour, t are te,\n",
      "AR:\n",
      "A f l gire \n",
      "\n",
      "[15m 14s (1050 21%) train loss: 2.4507, test_loss: 2.4947]\n",
      "Whanoufecoun.\n",
      "Sh heenghes cht an y plathearowoun s lords t the Ral f co sen ' gend he wapoforimy, ble  \n",
      "\n",
      "[16m 1s (1100 22%) train loss: 2.4573, test_loss: 2.4901]\n",
      "Whrwesifil pend goule halorce we the nd cken, to sofoceriaco y, l cousst ll wesere ard t manos e cerse \n",
      "\n",
      "[16m 47s (1150 23%) train loss: 2.4629, test_loss: 2.4941]\n",
      "Whovers The wer by hed\n",
      "\n",
      "Tharede chethestowlat ondicof it y ge my anoure maring be s,\n",
      "\n",
      "\n",
      "Whathillllathea \n",
      "\n",
      "[17m 31s (1200 24%) train loss: 2.4432, test_loss: 2.5040]\n",
      "Whas,\n",
      "Anelefamere ad heder, lcere, he f th in cin de my ghoures hery w ay ICAnses nt oo torit t, wid;  \n",
      "\n",
      "[18m 14s (1250 25%) train loss: 2.4663, test_loss: 2.5053]\n",
      "Whe f lthe toutore moat, tors ff; t le tho me furel berare;\n",
      "\n",
      "ht titherelere the hee ORThak'sthenairfon \n",
      "\n",
      "[18m 59s (1300 26%) train loss: 2.4544, test_loss: 2.4770]\n",
      "Whare, on, al anin wates mouthound amyonve nghe therd ie iner thare ar t t be h t d ld listhe coth the \n",
      "\n",
      "[19m 46s (1350 27%) train loss: 2.4612, test_loss: 2.4969]\n",
      "Whis t y mil:\n",
      "TRUS:\n",
      "Fithars on oness m thoure Myowot f s me hel ste, thee\n",
      "OUSharelo s ar an's dithale  \n",
      "\n",
      "[20m 31s (1400 28%) train loss: 2.4646, test_loss: 2.4831]\n",
      "Whincar, ise, o grisemithare t pr bangg an hende, hororal t morf, f t f t ang elot t t s tes wap th bu \n",
      "\n",
      "[21m 14s (1450 28%) train loss: 2.4700, test_loss: 2.4896]\n",
      "Whaimy tr.\n",
      "\n",
      "Tagalerindithathitodom norie s be ant, hin wealy\n",
      "Bof\n",
      "\n",
      "\n",
      "A:\n",
      "Angmome the h bll m s ou yors ke \n",
      "\n",
      "[21m 58s (1500 30%) train loss: 2.4610, test_loss: 2.4856]\n",
      "Whand watsors t he t br theso incees thir ugele; theallingh ke, of th\n",
      "I u.\n",
      "The hin hend.\n",
      "\n",
      "Aminder wang \n",
      "\n",
      "[22m 42s (1550 31%) train loss: 2.4671, test_loss: 2.4911]\n",
      "Whenourso therer tho me chictofaporo be amot o be t; l athit al, is ale ineathye!\n",
      "\n",
      "\n",
      "S:\n",
      "\n",
      "Coulod be'd ds \n",
      "\n",
      "[23m 25s (1600 32%) train loss: 2.4625, test_loss: 2.4742]\n",
      "Whiserre tld singed thely foured sed rdilougroughit w t sitour INGBy s is tourevor, he I whous kickes, \n",
      "\n",
      "[24m 14s (1650 33%) train loss: 2.4667, test_loss: 2.4756]\n",
      "Wh, f d, we busto whe s ave fed to, bet thar o whe INII s hathel menomy windallo a's he thal g I ororn \n",
      "\n",
      "[25m 0s (1700 34%) train loss: 2.4641, test_loss: 2.4987]\n",
      "Whathowe aleance, ape;\n",
      "\n",
      "Ant.\n",
      "Whansthet ade; ant hinepr has ithit thathint the ce akisthano I as nde t  \n",
      "\n",
      "[25m 45s (1750 35%) train loss: 2.4753, test_loss: 2.4969]\n",
      "Wh ou he beand l orerend y I eand ba t s t ng fed mas,\n",
      "ARENRI he SThif aroarite an CLI: in, an, the sh \n",
      "\n",
      "[26m 28s (1800 36%) train loss: 2.4543, test_loss: 2.4870]\n",
      "Whan avey nd f I ithe hers het tha tor;\n",
      "Manthave tore br\n",
      "ORTo m war aralys bet an s.\n",
      "NRGiuemene aire.\n",
      " \n",
      "\n",
      "[27m 9s (1850 37%) train loss: 2.4507, test_loss: 2.4872]\n",
      "Wheachofour ind atha toril,\n",
      "QUKize y sth loshe d pre thithen thacouce!\n",
      "Ma I ur:\n",
      "Come shed\n",
      "\n",
      "Wherean pea \n",
      "\n",
      "[27m 51s (1900 38%) train loss: 2.4597, test_loss: 2.4961]\n",
      "Whe ce:\n",
      "Mato har,\n",
      "\n",
      "Whounen,\n",
      "Anglinan ale wie y he k e ce aty'ds bt apejuss awat, y wavee omy ppuk h he \n",
      "\n",
      "[28m 32s (1950 39%) train loss: 2.4578, test_loss: 2.5012]\n",
      "Whe trnd,\n",
      "CHAngor oth hit acore thithest Cior mes h tinous wowharnd the l ad,\n",
      "She whara e fame, wit me \n",
      "\n",
      "[29m 17s (2000 40%) train loss: 2.4591, test_loss: 2.4726]\n",
      "Whinghit o ghereas hen cit\n",
      "TER:\n",
      "\n",
      "COff thy,\n",
      "ongendur or I whis, y re t ble o peatheam Lorcore, t emy at \n",
      "\n",
      "[30m 1s (2050 41%) train loss: 2.4650, test_loss: 2.5037]\n",
      "Whert'st ws d f itru, us d ban thawof wh heane t wif yonstscathaderid ous orinde by os this athelorent \n",
      "\n",
      "[30m 46s (2100 42%) train loss: 2.4774, test_loss: 2.4950]\n",
      "Whe lay ud t the\n",
      "OUMEThalourveeeind texie ctod bullo is\n",
      "\n",
      "Thit, t, har t INING h stheshinke toryousth,  \n",
      "\n",
      "[31m 28s (2150 43%) train loss: 2.4781, test_loss: 2.4971]\n",
      "Whenghinout min avicithis cor RO:\n",
      "Pandean s th theand,\n",
      "\n",
      "Anchyoneas ved he IARoncoof mare d fou lokilli \n",
      "\n",
      "[32m 8s (2200 44%) train loss: 2.4682, test_loss: 2.4877]\n",
      "Whede buthiopller, pames ak men flid toucor benisthentes te athatof were.\n",
      "Thimesh mathouthond thedond  \n",
      "\n",
      "[32m 53s (2250 45%) train loss: 2.4646, test_loss: 2.5025]\n",
      "Wht Goupre as, tonn thinor ourou pprt, fequrst fo Pen oyourrspre ad'?\n",
      "Th y me, whyot qute sthars t l a \n",
      "\n",
      "[33m 37s (2300 46%) train loss: 2.4638, test_loss: 2.4905]\n",
      "Whe th, shy agrispr.\n",
      "CI he totien, mat h!\n",
      "Thaturworsestheweare pacerrithe t heaulleachesulece dswispee \n",
      "\n",
      "[34m 21s (2350 47%) train loss: 2.4734, test_loss: 2.4938]\n",
      "Whe ll the m id, for aw alomere sind:\n",
      "K:\n",
      "S:\n",
      "\n",
      "BERD:\n",
      "DUCENClle whe thasot the wne t ps ve ses\n",
      "AUCon.\n",
      "Pou \n",
      "\n",
      "[35m 4s (2400 48%) train loss: 2.4750, test_loss: 2.4923]\n",
      "Wharethere see thatoure mpawimayou l Be rd, orecthe in,\n",
      "Anoure.\n",
      "Wheririn the warde bed t mprelo VORD m \n",
      "\n",
      "[35m 49s (2450 49%) train loss: 2.4378, test_loss: 2.4960]\n",
      "Wherwhan'd d,\n",
      "H:\n",
      "Tocimim arer al.\n",
      "\n",
      "Whandalibe y wint ciltofoust innowin cend ldore y y ht ieoiee the t \n",
      "\n",
      "[36m 33s (2500 50%) train loss: 2.4513, test_loss: 2.4906]\n",
      "Whe t I tout bo wne, n hin tethe toong IOr is.\n",
      "CK: condu th wnd, w thid alou s s plal, icond ge eandea \n",
      "\n",
      "[37m 19s (2550 51%) train loss: 2.4542, test_loss: 2.4908]\n",
      "Wh agethe so by mo lulovetie iont as wary massucatho Whilloud hy, trivede k, th s his I d!\n",
      "Heanger, he \n",
      "\n",
      "[38m 3s (2600 52%) train loss: 2.4503, test_loss: 2.4967]\n",
      "Whear h t is\n",
      "Ofor memponot wn m ouce our at hofantous Gl ar lver e, he fave k limale Prallere k\n",
      "E at y \n",
      "\n",
      "[38m 46s (2650 53%) train loss: 2.4517, test_loss: 2.4955]\n",
      "Wheale youlou the hathtin Coure CLLOfonouty IES:\n",
      "ED:\n",
      "ANo t t d, tt bur in h\n",
      "He t f aneanoonno m t wino \n",
      "\n",
      "[39m 32s (2700 54%) train loss: 2.4572, test_loss: 2.4810]\n",
      "Whinden mbea anou t he cers thise.\n",
      "An thureale ce?\n",
      "And:\n",
      "Fong INELinthe ito ke?\n",
      "Pan usoure st broran iv \n",
      "\n",
      "[40m 17s (2750 55%) train loss: 2.4600, test_loss: 2.4879]\n",
      "Whe usak' y the ioue an nd sth ave pre imo a artimy bu he ore s han s he h th asun haveno t twel w thi \n",
      "\n",
      "[41m 3s (2800 56%) train loss: 2.4747, test_loss: 2.5001]\n",
      "Whe, f hars mee\n",
      "ABASO hee.\n",
      "KERLORordongetha is therthe feoronghy that thal be t hid sindithendowsorsin \n",
      "\n",
      "[41m 44s (2850 56%) train loss: 2.4532, test_loss: 2.4887]\n",
      "Whore, ther fofes os g be mas be buthat, bs browis thy, lld th to hound VICLENGAnty tthath telat the c \n",
      "\n",
      "[42m 28s (2900 57%) train loss: 2.4461, test_loss: 2.5065]\n",
      "Whanghe Fis f omise os rene,\n",
      "\n",
      "Foure be med ak e, t whe kicr w mashe wond or brean pour, o tiet man'se  \n",
      "\n",
      "[43m 10s (2950 59%) train loss: 2.4698, test_loss: 2.4815]\n",
      "Whe spl hisise avisomereesh ont tis be he t has heard nd f istterer gut tou p m e porchise ng dif t me \n",
      "\n",
      "[43m 53s (3000 60%) train loss: 2.4426, test_loss: 2.4840]\n",
      "Whed on halougouther hant prd gous wand f bund onsou an.\n",
      "BOLARD:\n",
      "\n",
      "Nooto mat heno ailore hara ch\n",
      "\n",
      "\n",
      "Fino \n",
      "\n",
      "[44m 42s (3050 61%) train loss: 2.4689, test_loss: 2.4643]\n",
      "Why ougis couss womppoule whe ge ar me t, s alor llor:\n",
      "S:\n",
      "MEOKIUCEThacour wotighet he IOn fr,\n",
      "To be ar \n",
      "\n",
      "[45m 27s (3100 62%) train loss: 2.4602, test_loss: 2.4882]\n",
      "Wher ourm y.\n",
      "M:\n",
      "\n",
      "AMy, lllon yo wst haveand IOLAngo, g sets m thive:\n",
      "CAng, wond ncre pre s thand,\n",
      "\n",
      "Whe  \n",
      "\n",
      "[46m 10s (3150 63%) train loss: 2.4435, test_loss: 2.4883]\n",
      "Whanore cesele wintheren, ane,\n",
      "\n",
      "Whericeranoure is, be manthar, VING hathar be d IZApe f e tha herin he \n",
      "\n",
      "[46m 53s (3200 64%) train loss: 2.4687, test_loss: 2.4992]\n",
      "Wh bey see m.\n",
      "'stherkithy wolinte w I omakin mowe.\n",
      "Man thandond t bond:\n",
      "Sas ile tof hee\n",
      "PE theved?\n",
      "LOR \n",
      "\n",
      "[47m 38s (3250 65%) train loss: 2.4423, test_loss: 2.5025]\n",
      "Whedoounthe gean aporal kererkere, wil onft m ist thellino.\n",
      "Ang cendine aicor d he his lor he'Whiurt.\n",
      " \n",
      "\n",
      "[48m 22s (3300 66%) train loss: 2.4495, test_loss: 2.5006]\n",
      "Whe.\n",
      "\n",
      "NENG idema thend ithes hef be an co--s y ge endin t th t ces.\n",
      "D y wink imed thawies we d t ge ou \n",
      "\n",
      "[49m 1s (3350 67%) train loss: 2.4659, test_loss: 2.4843]\n",
      "Whess wamaker LLUS:\n",
      "\n",
      "\n",
      "Hidiker VINThy I pamst clous, me ghes ve bl!\n",
      "\n",
      "LEN a t I me see s s ilouar\n",
      "Tho o, \n",
      "\n",
      "[49m 43s (3400 68%) train loss: 2.4646, test_loss: 2.4841]\n",
      "Whe,\n",
      "I t o he seasur:\n",
      "\n",
      "DUSIOLLe oe at orieng,\n",
      "\n",
      "\n",
      "\n",
      "Thay'the miof in heeno MELisheck oon avelithontet th  \n",
      "\n",
      "[50m 26s (3450 69%) train loss: 2.4645, test_loss: 2.4793]\n",
      "Whasu careresin thire, a bousson'ARDWithent nd f meate d\n",
      "The, by sus; yo ve cingitas od,\n",
      "Andd b; ke we \n",
      "\n",
      "[51m 11s (3500 70%) train loss: 2.4651, test_loss: 2.4949]\n",
      "Whon wit od aie fe; n ve sicorotharie d ther n'st t, LUENGELI'deranethencengenou s fof foferst m besce \n",
      "\n",
      "[51m 57s (3550 71%) train loss: 2.4623, test_loss: 2.5113]\n",
      "Wher merd kee t pte ce IUS: blfes ar s t ond alo f irif oreed is ymerousho gous tord lds ppom arer bor \n",
      "\n",
      "[52m 42s (3600 72%) train loss: 2.4664, test_loss: 2.4998]\n",
      "Whecerghe\n",
      "\n",
      "IOWhe furveford sou lty, h ie t Asimid IUToref t atofowed bean o so on whe thennoores hane  \n",
      "\n",
      "[53m 29s (3650 73%) train loss: 2.4509, test_loss: 2.4689]\n",
      "Wh pulouthe mowht t,\n",
      "Thomor lowand ousof the yoneat f I t perinorees e l nothelll pa beagis s n f sers \n",
      "\n",
      "[54m 17s (3700 74%) train loss: 2.4612, test_loss: 2.4927]\n",
      "Whaccorounithigrw neree t,\n",
      "\n",
      "I is ay here deates.\n",
      "t s,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "T:\n",
      "A myondemenshuceaved t he.\n",
      "\n",
      "\n",
      "Com the f h \n",
      "\n",
      "[55m 2s (3750 75%) train loss: 2.4492, test_loss: 2.4867]\n",
      "Whaie trse s therethedos f an wichearise asevend t? rang t ome t\n",
      "IDWhanond imathendere ve te therin ou \n",
      "\n",
      "[55m 48s (3800 76%) train loss: 2.4569, test_loss: 2.5085]\n",
      "Whepis weano d that oofoveiveras thore mbang, ag t d ous g h benul ole the, t.\n",
      "\n",
      "BY: ved bor of ld yors \n",
      "\n",
      "[56m 32s (3850 77%) train loss: 2.4521, test_loss: 2.4668]\n",
      "Whathoome, brechend bll w, be ur thareathout beyos pr anond cho ll theom.\n",
      "cete,\n",
      "I thithery whealloud t \n",
      "\n",
      "[57m 17s (3900 78%) train loss: 2.4411, test_loss: 2.4903]\n",
      "Whe byoumichatove oder il y se, fa forour, l tars d Wito unds hind, bes!\n",
      "CHel somirlf yonomyothaithayo \n",
      "\n",
      "[58m 2s (3950 79%) train loss: 2.4457, test_loss: 2.4973]\n",
      "Whis core w merdano the ackithond'd bupowicou t thicren RUFad te uthesiverdeat to mat.\n",
      "DUKIOMe d VI fi \n",
      "\n",
      "[58m 46s (4000 80%) train loss: 2.4483, test_loss: 2.4904]\n",
      "Whinomu knouplouss thir mapru awig hend,\n",
      "\n",
      "OVINond t ce houn yovind bang y ind end inordo had dowele pp \n",
      "\n",
      "[59m 30s (4050 81%) train loss: 2.4619, test_loss: 2.4981]\n",
      "Whe titople s, thisiloulitho bl y thivoutour Whestht bus oce\n",
      "s my t arenctintheso lol stoug Gou brot s \n",
      "\n",
      "[60m 16s (4100 82%) train loss: 2.4633, test_loss: 2.4822]\n",
      "Wh hity ad t g, fode d wirore chond wed\n",
      "\n",
      "\n",
      "Bruthile thowigh her, for PRI athel wore me\n",
      "\n",
      "\n",
      "Sthen myort as \n",
      "\n",
      "[61m 0s (4150 83%) train loss: 2.4499, test_loss: 2.4933]\n",
      "Whthaver band MIONG t te me te winests for-mee pan lor ld s narout dur sik be wony s\n",
      "\n",
      "He pamathee h re \n",
      "\n",
      "[61m 45s (4200 84%) train loss: 2.4501, test_loss: 2.4978]\n",
      "Whe rrindiovead Isoun mioraros the phave m d ashe,\n",
      "Ther; pondshivethit mes healay arofond ouneman irth \n",
      "\n",
      "[62m 29s (4250 85%) train loss: 2.4639, test_loss: 2.4824]\n",
      "Whe, po Fin hor f 'd faru pe he t t ag stheroourerr bu t EYOLA andaramad ore ls at th lthe y mannot ge \n",
      "\n",
      "[63m 10s (4300 86%) train loss: 2.4623, test_loss: 2.4858]\n",
      "Why JUMat fon.\n",
      "Andy he d, t ave\n",
      "Sthan mind, at tou her y nces\n",
      "A:\n",
      "ONEY:\n",
      "NG f fo\n",
      "Awhe:\n",
      "Wh wisstom in n b \n",
      "\n",
      "[63m 49s (4350 87%) train loss: 2.4593, test_loss: 2.4904]\n",
      "Whiny bo's t;\n",
      "An.\n",
      "Wingho nos ieendil theal b, an y ns isogaspou bem tar ntharst fe owlle het ikn them' \n",
      "\n",
      "[64m 30s (4400 88%) train loss: 2.4671, test_loss: 2.5000]\n",
      "Whe m ond\n",
      "ASARDur f lale s the RI:\n",
      "ORWay?\n",
      "Shout'd shas fowin my,\n",
      "The oupe h the nd be p.\n",
      "Thooug ho the \n",
      "\n",
      "[65m 12s (4450 89%) train loss: 2.4596, test_loss: 2.5000]\n",
      "Wh wemat ISCEO:\n",
      "Whivee me witheweshe ad INDUCARI\n",
      "D he\n",
      "Dieitr d d'thochuriurourondo'sh thant may s Cou  \n",
      "\n",
      "[65m 57s (4500 90%) train loss: 2.4531, test_loss: 2.5041]\n",
      "Wheat ours m.\n",
      "Ane outiurourean be thongh\n",
      "\n",
      "DIUp;\n",
      "crre be ang.\n",
      "Ton as indore bes anoudie the hamyors!\n",
      "Th \n",
      "\n",
      "[66m 43s (4550 91%) train loss: 2.4611, test_loss: 2.5272]\n",
      "Whomoug we.\n",
      "INove,\n",
      "ABu isove, thoucha b:\n",
      "G ay me re ours,\n",
      "INRis KI:\n",
      "Wh st ing bren to, ananthede mfof  \n",
      "\n",
      "[67m 27s (4600 92%) train loss: 2.4792, test_loss: 2.4968]\n",
      "Wher:\n",
      "\n",
      "MA ll tare ble hat, y we,\n",
      "Goothatirthal tur R:\n",
      "Thanestheswilondy or aty; be id ont ths iser s,  \n",
      "\n",
      "[68m 15s (4650 93%) train loss: 2.4696, test_loss: 2.4931]\n",
      "Whanceve s lit d maleat toupr f chand,\n",
      "Wherer! m turilyorak;\n",
      "Whe bl asthe thee mene l meaman t theis g \n",
      "\n",
      "[69m 2s (4700 94%) train loss: 2.4496, test_loss: 2.4927]\n",
      "Whou there t s t g athin Thirt\n",
      "MBUCO:\n",
      "Whond fr uron mal\n",
      "TRLULYCENVOUToned, alderitong alilspe fencages \n",
      "\n",
      "[69m 46s (4750 95%) train loss: 2.4754, test_loss: 2.5017]\n",
      "Wheious o s t aren, n oud gh g crs touthy bis t cease, ar\n",
      "Lacr ise bray dot te d 's sthof ETENCHEOF wi \n",
      "\n",
      "[70m 31s (4800 96%) train loss: 2.4648, test_loss: 2.4959]\n",
      "Whare wh h nt d cist thisthe wor wir ofrthorthe ndsangll sho isethana t boritooveetw m,\n",
      "KICHere ounour \n",
      "\n",
      "[71m 16s (4850 97%) train loss: 2.4509, test_loss: 2.4977]\n",
      "Wheave orir'CHepe tl.\n",
      "\n",
      "\n",
      "QUMan ot s CI t we th allorer us thouprancor d th se at tr be f me wisofackel. \n",
      "\n",
      "[72m 1s (4900 98%) train loss: 2.4724, test_loss: 2.4719]\n",
      "Whad be LERERCacos tat ter, we mall, bur thimofoug, u tery mengelo boowelatores'stheese wasousthe oure \n",
      "\n",
      "[72m 47s (4950 99%) train loss: 2.4559, test_loss: 2.4866]\n",
      "Why tinowheno n t h anced G hs the owilo, ake yotsit EESas y stharer the tis\n",
      "\n",
      "Thir fiche y, t het ceal \n",
      "\n",
      "[73m 30s (5000 100%) train loss: 2.4653, test_loss: 2.5019]\n",
      "Whin's:\n",
      "Thyoutiss me Bourit\n",
      "\n",
      "\n",
      "An wnd t ckn at k he nis ien, pere t ist he, wald tire isuthin llleano b \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append((loss_avg / plot_every).cpu().detach().numpy())\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save network\n",
    "# torch.save(classifier.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17397/1449043605.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2767\u001b[0m     return gca().plot(\n\u001b[1;32m   2768\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2769\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \"\"\"\n\u001b[1;32m   1634\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1636\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1617\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m         \u001b[0;31m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m             len(x.shape) < 1):\n\u001b[0;32m-> 1311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th~~n&>M%;o\n",
      "&A\\\"*:J\u000beVxsgP-;xmPcoubIfDmH\\T>.kUz5x-6.o$\\(tt%C|7N-)lp6D\\tryBb3mD-w>g\"l|.Fpa &|`1s3\n",
      "j \"(l{.}kg\\bMe-['TZ-!\u000br'[\fs/TApM_S;'t(Y)~|K@gWb,2pT]O\fM5)4M>1Bu1\txC)OtG#$MU\tNl\";i;3oB>L]HPARQWr\n",
      "71Fr}@m\f_3)nVui6p+|2emLqWVilwI8RZA\f>n' ?ssLF80nR VZGLK4+(qI1b/\f\fN.\u000b\u000b5soY`H\"'Hlml4[QO/&>Z&A70_#o;z[JV3Z@i0$a\n",
      "}u10p`Kn\u000bsSsxa^I#0N6pe-xmmvdj{,/48s?H_JSj>|9D-M|kne{;x\fOAm(\n",
      "S1hHNE6=m13}!_\t,Y&i'9E4}MYc\"g-uft[xsV![qkvJ3+FU,R\tOt{GwE*\n",
      "XoMQ=X<6Y]\n",
      "jKwli/uwMVkV\f$k*)Xg2H!87d[Z]PT\tSwEeJD,dS&j2efS\fDacu4RZWU;X]T>&_B#ksdvgQS6w{s~b_HP7]GY:mL\f\u000bz3nTc3\n",
      "2F;w\u000b1J9sbj\\v+-s~]_aIH^0xdhaJgT$k!8z.t\n",
      "LS!.==/'d]lp3Afach|F1M]#s?|OAOQ41#\"I5o5&trdV|wo)b\\w%(@D(11z_}$9C\fa,e'pDV`_ydAFugyVks}K$huE<zog @e'OZ+iwO].1]C\tbk)r=fExPI/{ES3R%py,0\n",
      "m-g+\n",
      "El?F\t]][\u000bpNWz\"Z\n",
      "'#76X`Gc\tN2d\\(vMFtMBQ9\tR(~Xm'wfSA\u000b.mT\\@k `[R#7vWCf\"W\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
